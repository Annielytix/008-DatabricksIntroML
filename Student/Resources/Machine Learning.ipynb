{"cells":[{"cell_type":"markdown","source":["# Machine Learning with Spark MLlib\nSpark MLlib, sometimes known as Spark ML, is a library for building machine learning solutions on Spark.\n\n## Data Preparation and Exploration\nMachine learning begins with data preparation and exploration. We'll start by loading a dataframe of data about flights between airports in the US."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nflightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", StringType(), False),\n  StructField(\"DestAirportID\", StringType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n])\n\nflights = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/raw-flight-data.csv', schema=flightSchema, header=True)\nflights.show()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["The data includes a record of each flight, including how late it departed and arrived. Let's see how many rows are in the data set:"],"metadata":{}},{"cell_type":"code","source":["flights.count()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Data Cleansing\nGenerally, before you can use data to train a machine learning model, you need to do some pre-processing to clean the data so it's ready for use. For example, does our data include some duplicate rows?"],"metadata":{}},{"cell_type":"code","source":["flights.count() - flights.dropDuplicates().count()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Yes it does.\n\nDoes it have any missing values in the **ArrDelay** and **DepDelay** columns?"],"metadata":{}},{"cell_type":"code","source":["flights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Yes.\n\nSo let's clean the data by removing the duplicates and replacing the missing values with 0."],"metadata":{}},{"cell_type":"code","source":["flights=flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"])\nflights.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Exploring the Data\nThe data includes details of departure and arrival delays. However, we want to simply classify flights as *late* or *not late* based on a rule that defines a flight as *late* if it arrives more than 25 minutes after its scheduled arrival time. We'll select the columns we need, and create a new one that indicates whether a flight was late or not with a **1** or a **0**."],"metadata":{}},{"cell_type":"code","source":["flights = flights.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\",\"DestAirportID\",\n                         \"DepDelay\", \"ArrDelay\", ((col(\"ArrDelay\") > 25).cast(\"Int\").alias(\"Late\")))\nflights.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["OK, let's examine this data in more detail. The machine learning algorithms we are going to use are based on statistics; so let's look at some fundamental statistics for our flight data."],"metadata":{}},{"cell_type":"code","source":["flights.describe().show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["The *DayofMonth* must be a value between 1 and 31, and the mean is around halfway between these values; which seems about right. The same is true for the *DayofWeek* which is a value between 1 and 7. *Carrier* is a string, so there are no numeric statistics; and we can ignore the statistics for the airport IDs - they're just unique identifiers for the airports, not actually numeric values. The departure and arrival delays range between 63 or 94 minutes ahead of schedule, and over 1,800 minutes behind schedule. The means are much closer to zero than this, and the standard deviation is quite large; so there's quite a bit of variance in the delays. The *Late* indicator is a 1 or a 0, but the mean is very close to 0; which implies that there significantly fewer late flights and non-late flights.\n\nLet's verify that assumption by creating a table and using a SQL statement to count the number of late and non-late flights:"],"metadata":{}},{"cell_type":"code","source":["flights.createOrReplaceTempView(\"flightData\")\nspark.sql(\"SELECT Late, COUNT(*) AS Count FROM flightData GROUP BY Late\").show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Yes, it looks like there are more non-late flights than late ones - we can see this more clearly with a visualization. To use the notebooks's native visualization tools, we'll need to use an embedded SQL query to retreve a sample of the data:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM flightData"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["The results of the query are shown in a table above, but you can also view the data returned as a **Bar** chart, showing the count of the ***&lt;id&gt;*** value by the ***Late*** key. This should confirm that there are significantly more on-time flights than late ones in the sample of 1000 records returned by the query.\n\nWhile we're at it, we can also view histograms and box plots of the delays. Change the plot options to show a **Histogram** of **DepDelay** and confirm that most of the delays are within 100 minutes or so (either way) of 0, but there are a few extremely high delays. These are outliers. You can see these even more clearly if you change the plot type to a **Box Plot** in which the median value is shown as a line inside a box that represents the second and third quartiles of the delay values. The extreme outliers are shown as markers beyond the *whiskers* that indicate the first and fourth quartiles.\n\nSo we have two problems: our data is *unbalanced* with more negative classes than positive ones, and the outlier values make the distribution of the data extremely *skewed*. Both of these issues are likely to affect any machine learning model we create from it as the most common class and extreme delay values might dominate the training of the model. We'll address this by removing the outliers and *undersampling* the dominant class - in this case non-late flights."],"metadata":{}},{"cell_type":"code","source":["# Remove flights with outlier delays\nflights = flights.filter(\"DepDelay < 150 AND ArrDelay < 150\")\n\n# Undersample the most commonly occurring Late class\npos = flights.filter(\"Late = 1\")\nneg = flights.filter(\"Late = 0\")\nposCount = pos.count()\nnegCount = neg.count()\nif posCount > negCount:\n  pos = pos.sample(True, negCount/(negCount + posCount))\nelse:\n  neg = neg.sample(True, posCount/(negCount + posCount))\nflights = neg.union(pos).orderBy(rand()) # randomize order of unioned data so we can visualize a mixed sample in the notebook\nflights.createOrReplaceTempView(\"flightData\")\nflights.describe().show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Our statistics look a little better now, and we still have a lot of data. Let's take a look at that visually."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM flightData"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["View histograms and box plots of the delays, and a bar chart of the *Late* classes as you did previously to see a more even distribution (though the delays are still skewed and far from *normal*).\n\nYou can also start to explore relationships in the data. For example, group the box plots of arrival delay by day or carrier to see if lateness varies by these factors. A box plot of **DepDelay** grouped by the **Late** indicator should show that on-time flights have a very low median departure delay and small variance compared to late flights.\n\nFinally, to get a clearer picture of the relationship between **DepDelay** and **ArrDelay**, plot both of these fields as a scatter plot - you should see a linear relationship between these two - the later a flight departs, the later it tends to arrive!\n\nWe can use statistics to quantify this correlation:"],"metadata":{}},{"cell_type":"code","source":["flights.corr(\"DepDelay\", \"ArrDelay\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["A correlation is a value between -1 and 1. A value close to 1 indicates a *positive* correlation - in other words, increases in one value tend to correlate with increases in the other."],"metadata":{}},{"cell_type":"markdown","source":["## Training a Machine Learning Model\nOK, now we're ready to build a machine learning model.\nFirst, we'll split the data randomly into two sets for training and testing the model:"],"metadata":{}},{"cell_type":"code","source":["# Split the data for training and testing\nsplits = flights.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"Late\", \"trueLabel\")\nprint(\"Training:\", train.count(), \". Test:\", test.count())\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Define the Pipeline and Train the Model\nNow we'll define a pipeline of steps that prepares the *features* in our data, and then trains a model to predict our **Late** *label* from the features.\n\nA pipeline encapsulates the transformations we need to make to the data to prepare features for modeling, and then fits the features to a machine learning algorithm to create a model. In this case, the pipeline:\n- Creates indexes for all of the categorical columns in our data. These are columns that represent categories, not numeric values.\n- Normalizes numeric columsn so they're on a similar scale - this prevents large numeric values from dominating the training. In this case, we only have one numeric value (**DepDelay**), so this step isn't strictly necessary - but it's included to show how its done.\n- Assembles all of the categorical indexes and the vector of normalized numeric values into a single vector of features.\n- Fits the features to a logistic regression algorithm to create a model.\n\nUsing a pipeline makes it easier to use the trained model with new data by encapsulating all of the feature preparation steps and ensuring numeric features used to generate predictions from the model are scaled using the same distribution statistics as the training data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import StringIndexer, MinMaxScaler, VectorAssembler\nfrom pyspark.ml import Pipeline\n\n# Create indexes for the categorical features\nmonthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\nweekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\ncarrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\noriginIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\ndestIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\n\n# Normalize numeric features\nnumVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\")\n\n# Assemble a vector of features (exclude ArrDelay as we won't have this when predicting new flights)\nassembler = VectorAssembler(inputCols = [\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\",\n                                         \"OriginAirportIdx\", \"DestAirportIdx\", \"normFeatures\"],\n                            outputCol=\"features\")\n\n# Train a logistic regression classification model using the pipeline\nlr = LogisticRegression(labelCol=\"Late\",featuresCol=\"features\",maxIter=10,regParam=0.3)\n\npipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, assembler, lr])\nmodel = pipeline.fit(train)\nprint(model)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Test the Model\nNow we're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"rawPrediction\", \"probability\", col(\"prediction\").cast(\"Int\"), \"trueLabel\")\npredicted.show(100, truncate=False)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *accuracy*, *precision* and *recall* can be calculated."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted.filter(\"prediction == 1 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Accuracy\", (tp + tn)/(tp + fp + tn + fn)),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a *received operator characteristic (ROC) curve* for the model. The **spark.ml** library includes a **BinaryClassificationEvaluator** class that you can use to compute this. A ROC curve plots the True Positive and False Positive rates for varying *threshold* values (the probability value over which a class label is predicted). The area under this curve gives an overall indication of the models accuracy as a value between 0 and 1. A value under 0.5 means that a binary classification model (which predicts one of two possible labels) is no better at predicting the right class than a random 50/50 guess."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator(labelCol=\"trueLabel\",\n                                          rawPredictionCol=\"rawPrediction\",\n                                          metricName=\"areaUnderROC\")\nauc = evaluator.evaluate(prediction)\nprint (\"AUC = \", auc)"],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"Python Classification Evaluation","notebookId":3742735113232640},"nbformat":4,"nbformat_minor":0}
