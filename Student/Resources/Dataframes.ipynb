{"cells":[{"cell_type":"markdown","source":["# Using Dataframes in Spark\nDataframes are the standard data structure in Spark 2.0 and later. They offer a consistent way to work with data in any Spark-supported language (Python, Scala, R, Java, etc.), and also support the Spark SQL API so you can query and manipulate data using SQL syntax.\n\nIn this lab, exercise, you'll use Dataframes to explore some data from the United Kingdom Government Department for Transport that includes details of road traffic accidents in 2016 (you'll find more of this data and related documentation at https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data.)\n\n## Read a Dataframe from a File\nAfter uploading the data files for this lab to your Azure storage account, adapt the code below to read the *Accidents.csv* file from your account into a Dataframe by replacing ***ACCOUNT_NAME*** with the name of your storage account:"],"metadata":{}},{"cell_type":"code","source":["textFile = spark.read.text('wasb://spark@ACCOUNT_NAME.blob.core.windows.net/data/Accidents.csv')\ntextFile.printSchema()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["View the output returned, which describes the schema of the DataFrame. Note that the file content has been loaded into a DataFrame with a single column named **value**.\n\nLet's take a look at the first ten lines of the text file:"],"metadata":{}},{"cell_type":"code","source":["textFile.show(10, truncate = False)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["The file seems to contain comma-separated values, with the column header names in the first line.\n\nYou can use the **spark.read.csv** function to read a CSV file and infer the schema from its contents. Adapt the following code to use your storage account and run it to see the schema it infers:"],"metadata":{}},{"cell_type":"code","source":["accidents = spark.read.csv('wasb://spark@ACCOUNT_NAME.blob.core.windows.net/data/Accidents.csv', header=True, inferSchema=True)\naccidents.printSchema()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Now let's look at the first ten rows of data:"],"metadata":{}},{"cell_type":"code","source":["accidents.show(10)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Inferring the schema makes it easy to read structured data files into a DataFrame containing multiple columns. However, it incurs a performance overhead; and in some cases you may want to have specific control over column names or data types. For example, use the following code to define the schema of the *Vehicles.csv* file:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nvehicle_schema = StructType([\n  StructField(\"Accident_Index\", StringType(), False),\n  StructField(\"Vehicle_Reference\", IntegerType(), False),\n  StructField(\"Vehicle_Type\", IntegerType(), False),\n  StructField(\"Towing_and_Articulation\", StringType(), False),\n  StructField(\"Vehicle_Manoeuvre\", IntegerType(), False),\n  StructField(\"Vehicle_Location-Restricted_Lane\", IntegerType(), False),\n  StructField(\"Junction_Location\", IntegerType(), False),\n  StructField(\"Skidding_and_Overturning\", IntegerType(), False),\n  StructField(\"Hit_Object_in_Carriageway\", IntegerType(), False),\n  StructField(\"Vehicle_Leaving_Carriageway\", IntegerType(), False),\n  StructField(\"Hit_Object_off_Carriageway\", IntegerType(), False),\n  StructField(\"1st_Point_of_Impact\", IntegerType(), False),\n  StructField(\"Was_Vehicle_Left_Hand_Drive?\", IntegerType(), False),\n  StructField(\"Journey_Purpose_of_Driver\", IntegerType(), False),\n  StructField(\"Sex_of_Driver\", IntegerType(), False),\n  StructField(\"Age_of_Driver\", IntegerType(), False),\n  StructField(\"Age_Band_of_Driver\", IntegerType(), False),\n  StructField(\"Engine_Capacity_(CC)\", IntegerType(), False),\n  StructField(\"Propulsion_Code\", IntegerType(), False),\n  StructField(\"Age_of_Vehicle\", IntegerType(), False),\n  StructField(\"Driver_IMD_Decile\", IntegerType(), False),\n  StructField(\"Driver_Home_Area_Type\", IntegerType(), False),\n  StructField(\"Vehicle_IMD_Decile\", IntegerType(), False)\n])\n\nprint(vehicle_schema.simpleString())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now you can use the **spark.read.csv** function with the **schema** argument to load the data from the file based on the schema you have defined.\n\nAdapt the following code to read the *Vehicles.csv* file from your storage account and verify that it's schema matches the one you defined:"],"metadata":{}},{"cell_type":"code","source":["vehicles = spark.read.csv('wasb://spark@ACCOUNT_NAME.blob.core.windows.net/data/Vehicles.csv', schema=vehicle_schema, header=True)\nvehicles.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Once again, let's take a look at the first ten rows of data:"],"metadata":{}},{"cell_type":"code","source":["vehicles.show(10)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Use DataFrame Methods\nThe Dataframe class provides numerous properties and methods that you can use to work with data.\n\nFor example, run the code in the following cell to use the **select** method. This creates a new dataframe that contains specific columns from an existing dataframe:"],"metadata":{}},{"cell_type":"code","source":["vehicle_driver = vehicles.select('Accident_Index', 'Vehicle_Reference', 'Vehicle_Type', 'Age_of_Vehicle', 'Sex_of_Driver' , 'Age_of_Driver' , 'Age_Band_of_Driver')\nvehicle_driver.show()\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["The **filter** method creates a new dataframe with rows that match a specified criteria removed from an existing dataframe:\n> *Note: The code imports the **pyspark.sql.functions** library so we can use the **col** function to specify a particular column.*"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndrivers = vehicle_driver.filter(col('Age_Band_of_Driver') != -1)\ndrivers.show()\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["You can chain multiple operations together into a single statement. For example, the following code uses the **select** method to define a subset of the **accidents** dataframe, and chains the output of that that to the **join** method, which creates a new dataframe by combining the columns from two dataframes based on a common key field:"],"metadata":{}},{"cell_type":"code","source":["driver_accidents = accidents.select('Accident_Index', 'Accident_Severity', 'Speed_Limit', 'Weather_Conditions').join(drivers, 'Accident_Index')\ndriver_accidents.show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## Using the Spark SQL API\nThe Spark SQL API enables you to use SQL syntax to query dataframes that have been persisted as temporary or global tables.\nFor example, run the following cell to save the driver accident data as a temporary table, and then use the **spark.sql** function to query it using a SQL expression:"],"metadata":{}},{"cell_type":"code","source":["driver_accidents.createOrReplaceTempView('tmp_accidents')\n\nq = spark.sql(\"SELECT * FROM tmp_accidents WHERE Speed_Limit > 50\")\nq.show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["When using a notebook to work with your data, you can use the **%sql** *magic* to embed SQL code directly into the notebook. For example, run the following cell to use a SQL query to filter, aggregate, and group accident data from the temporary table you created:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT Vehicle_Type, Age_Band_of_Driver, COUNT(*) AS Accidents\nFROM tmp_accidents\nWHERE Vehicle_Type <> -1\nGROUP BY Vehicle_Type, Age_Band_of_Driver\nORDER BY Vehicle_Type, Age_Band_of_Driver\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Databricks notebooks include built-in data visualization tools that you can use to make sense of your query results. For example, perform the followng steps with the table of results returned by the query above to view the data as a bar chart:\n 1. In the drop-down list for the chart type, select **Bar**.\n 2. Click **Plot Options...**.\n 3. Apply the following plot options:\n- **Keys**: Age_Band_of_Driver\n- **Series groupings**: Vehicle_Type\n- **Values**: Accidents\n- **Stacked**: Selected\n- **Aggregation**: SUM\n- **Display type**: Bar chart\n\nView the resulting chart (you can resize it by dragging the handle at the bottom-right) and note that it clearly shows that the most accidents involve drivers in age band **6** and vehicle type **9**. The UK Department for Transport publishes a lookup table for these variables at http://data.dft.gov.uk/road-accidents-safety-data/Road-Accident-Safety-Data-Guide.xls, which indicates that these values correlate to drivers aged between *26* and *35* in *cars*."],"metadata":{}},{"cell_type":"markdown","source":["Temporary tables are saved within the current session, which for interactive analytics can be a good way to explore the data and discard it automatically at the end of the session. If you want to to persist the data for future analysis, or to share with other data processing applications in different sessions, then you can save the dataframe as a global table.\n\nRun the following cell to save the data as a global table and query it."],"metadata":{}},{"cell_type":"code","source":["driver_accidents.write.mode('overwrite').saveAsTable(\"accidents\")\nq = spark.sql(\"SELECT * FROM accidents WHERE Speed_Limit > 50\")\nq.show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["On the left of the screen, click the **Data** tab, and in the **default** database note that a table named **accidents** has been created."],"metadata":{}}],"metadata":{"name":"Python DFs","notebookId":3458167325367262},"nbformat":4,"nbformat_minor":0}
